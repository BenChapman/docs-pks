---
title: Installing and Configuring PKS with NSX-T Integration
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to install and configure Pivotal Container Service (PKS) on vSphere with NSX-T integration.

##<a id='architecture'></a>Deployment Architecture

The instructions in this topic deploy NSX-T with PKS using the Network Address Translation (NAT) topology. The following figure shows the NAT deployment architecture:

![NSX & PKS Overview](images/nsx-t-overview.png)

Click [here](https://docs.pivotal.io/runtimes/pks/1-0/images/nsx-t-overview.png) to view a larger version of this image.

The topology has the following characteristics:

* The BOSH Director, Ops Manager, and the PKS service instance are all located on a logical switch NAT'd behind a T1 logical router.
* All Kubernetes cluster nodes are located on a logical switch NAT'd behind a T1 logical router. This will require NAT rules to allow access to Kubernetes APIs.

##<a id='before'></a>Before You Install

Before performing the procedures in this topic:

* Consult with the requirements described in [vSphere Prerequisites and Resource Requirements](vsphere-requirements.html).
* The instructions in this section are cumulative. For each step, be sure to follow instructions precisely. Complete any confirmation tasks described in the <a href="https://docs.vmware.com/en/VMware-NSX-T/index.html">NSX-T documentation</a> to verify your setup before proceeding to the next step. 
* Comply with any requirements or best practices described in the <a href="https://docs.vmware.com/en/VMware-NSX-T/index.html">NSX-T documentation</a>.</p>

<p class="note"><strong>Note</strong>: When using NSX-T 2.1, creating namespaces with names longer than 40 characters may result in a truncated/hashed name.</p>

##<a id='config-network'></a> Step 1: Pre-allocate Network Subnets

Create and assign the following network CIDRs in the IPv4 address space according to the instructions in the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html). Ensure that they are routable in your environment.

* **VTEP CIDR(s)**: One or more of these networks will host your GENEVE Tunnel Endpoints on your NSX Transport Nodes. Size the network(s) to support all of your expected Host and Edge Transport Nodes. For example, a CIDR of `192.168.1.0/24` will provide 254 usable IPs. This will be used when creating the `ip-pool-vteps` in Step 3.
* **PKS MANAGEMENT CIDR**: This small network will be used for NAT access to PKS management components such as Ops Manager and the PKS Service VM. For example, a CIDR of `10.172.1.0/28` will provide 14 usable IPs.
* **PKS LB CIDR**: This network will provide your load balancing address space for each Kubernetes cluster created by PKS. The network will also provide IP addresses for Kubernetes API access and Kubernetes exposed services. For example, `10.172.2.0/25` will provide 126 usable IPs. This network will be used when creating the `ip-pool-vips` described in [Create NSX Network Objects](#create-objects-network-objects).

Refer to the instructions in the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) to ensure that your network topology enables the following communications:

* vCenter, NSX-T components, and ESXi hosts must be able to communicate with each other.
* The Ops Manager Director VM must be able to communicate with vCenter and the NSX Manager.
* The Ops Manager Director VM must be able to communicate with all nodes in all Kubernetes clusters.
* Each Kubernetes cluster deployed by PKS will deploy a NCP pod that must be able to communicate with the NSX Manager.

##<a id='deploy-nsx-t'></a> Step 2: Deploy NSX-T

Deploy NSX-T according to the instructions in the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html).

<p class="note"><strong>Note</strong>: In general, accept default settings unless instructed otherwise.</p>


1. [Deploy](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-A65FE3DD-C4F1-47EC-B952-DEDF1A3DD0CF.html) the NSX Manager.
1. [Deploy](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-447C0417-A37B-4C2E-965E-499F52587160.html) NSX Controllers.
1. [Join](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-05434745-7D74-4DA8-A68E-9FE17093DA7B.html) the NSX Controllers to the NSX Manager and [initialize](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-273F6344-7212-4105-9FBA-A872CD75803F.html) the Control Cluster.
1. [Add](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-8C0EEC08-3A63-4918-A5E2-7A94AD50B0E6.html) your ESX host(s) to the NSX-T Fabric. Each host must have at least one **free nic/vmnic** not already used by other vSwitches on the ESX host for use with NSX Host Transport Nodes.
1. [Deploy](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-11417AA2-5EBC-49C7-8A86-EB94604261A6.html) NSX Edge VMs (recommended at least two). Each deployed NSX Edge VM will require free resources in your vSphere Environment to provide 8 vCPU, 16 GB of RAM, and 120 GB of storage. When deploying, you must connect the vNICs of the NSX Edge VMs to an appropriate PortGroup for your environment by completing the following steps:
  1. Connect the first Edge interface to your environment's PortGroup/VLAN where your Edge Management IP can route and communicate with the NSX Manager.
  1. Connect the second Edge interface to your environment's PortGroup/VLAN where your T0 uplink interface will be located.  Your **PKS MANAGEMENT CIDR** and **PKS LB CIDR** should be routable to this PortGroup.  
  1. Connect the third Edge interface to your environment's PortGroup/VLAN where your GENEVE VTEPs can route and communicate with each other. Your **VTEP CIDR** should be routable to this PortGroup.
  1. [Join](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-11BB4CF9-BC1D-4A76-A32A-AD4C98CBF25B.html) the NSX Edge VMs to the NSX-T Fabric.


##<a id='create-objects'></a> Step 3: Create the NSX-T Objects Required for PKS

Create the NSX-T objects (network objects, logical switches, NSX Edge, and logical routers) needed for PKS deployment according to the instructions in the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html). 

###<a id='create-objects-network-objects'></a> Create NSX Network Objects

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-E7F7322D-D09B-481A-BD56-F1270D7C9692.html) two NSX IP pools:
  * One NSX IP pool for GENEVE Tunnel Endpoints `ip-pool-vteps,` within the usable range of the **VTEP CIDR** created in Step 1, to be used with NSX Transport Nodes that you create later in this section
  * One NSX IP pool for NSX Load Balancing VIPs `ip-pool-vips,` within the usable range of the **PKS LB CIDR** created in Step 1, to be used with the T0 Logical Router that you create later in this section
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html) two NSX Transport Zones (TZs):
  * One NSX TZ for PKS control plane Services and Kubernetes Cluster deployment overlay network(s) called `tz-overlay` and the associated N-VDS `hs-overlay` (select Standard)
  * One NSX TZ for NSX Edge uplinks (ingress/egress) for PKS Kubernetes cluster(s) called `tz-vlan` and the associated N-VDS `hs-vlan` (select Standard)
1. If the default uplink profile is not applicable in your deployment, [create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-50FDFDFB-F660-4269-9503-39AE2BBA95B4.html) your own NSX uplink host profile.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-D7CA778B-6554-4A23-879D-4BC336E01031.html) NSX Host Transport Node(s): 
  * For each host in the NSX-T Fabric, create a node named `tnode-host-NUMBER`. For example, if you have three hosts in the NSX-T Fabric, create three nodes named `tnode-host-1`, `tnode-host-2`, and `tnode-host-3`.
  * Add the `tz-overlay` NSX Transport Zone to each NSX Host Transport Node.
  <p class="note"><strong>Note</strong>: The Transport Nodes must be placed on free host NICs not already used by other vSwitches on the ESX host. Use the `ip-pool-vteps` IP pool that will allow them to route and communicate with each other, as well as other Edge Transport Nodes, to build GENEVE tunnels.</p>

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-46C7B20D-4BE4-400E-AF39-1ADFE945DE38.html) an NSX IP Block named `ip-block-pks-deployments`. The NSX-T Container Plug-in (NCP) and PKS will use this IP Block to assign address space to Kubernetes pods through the Container Networking Interface (CNI). Pivotal recommends using the CIDR block `172.16.0.0/16`.

###<a id='create-objects-logical-switches'></a> Create Logical Switches

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-23194F9A-416A-40EA-B9F7-346B391C3EF8.html) the following NSX Logical Switches:
  * One for T0 ingress/egress uplink port `ls-pks-uplink` 
  * One for the PKS Management Network `ls-pks-mgmt`
  * One for the PKS Service Network `ls-pks-service`
1. Attach your first NSX Logical Switch to the `tz-vlan` NSX Transport Zone.
1. Attach your second and third NSX Logical Switches to the `tz-overlay` NSX Transport Zone.

###<a id='create-objects-create-nsx-edge'></a> Create NSX Edge Objects


1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-53295329-F02F-44D7-A6E0-2E3A9FAE6CF9.html) NSX Edge Transport Node(s).
1. Add both `tz-vlan` and `tz-overlay` NSX Transport Zones to the NSX Edge Transport Node(s). Controller Connectivity and Manager Connectivity should be **UP**.
1. Refer to the MAC addresses of the Edge VM interfaces you deployed to deploy your virtual NSX Edge(s):
  1. Connect the `hs-vlan` N-VDS to the vNIC (`fp-eth#`) that matches the MAC address of the second NIC from your deployed Edge VM.
  1. Connect the `hs-overlay` N-VDS to the vNIC (`fp-eth#`) that matches the MAC address of the third NIC from your deployed Edge VM. 
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.install.doc/GUID-898099FC-4ED2-4553-809D-B81B494B67E7.html) an NSX Edge cluster called `edge-cluster-pks`.
1. Add the NSX Edge Transport Node(s) to the cluster.

###<a id='create-objects-logical-routers'></a> Create Logical Routers

####<a id='create-objects-logical-router-pks'></a> Create T0 Logical Router for PKS

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-7891E6E7-606D-4F79-8AB7-BC01898F9FE7.html) a Tier-0 (T0) logical router named `t0-pks`: 
  * Select `edge-cluster-pks` for the cluster.   
  * Set **High Availability Mode** to **Active-Standby**. NAT rules will be applied on T0 by NCP. If not set **Active-Standby**, the router will not support NAT rule configuration.

1. [Attach](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) the logical router to the `ls-pks-uplink` logical switch you created previously. Create a logical router port for `ls-pks-uplink` and assign an IP address and CIDR that your environment will use to route to all PKS assigned IP pools and IP blocks.
1. [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-3F163DEE-1EE6-4D80-BEBF-8D109FDB577C.html) T0 routing to the rest of your environment using the appropriate routing protocol for your environment or by using static routes. The CIDR used in `ip-pool-vips` *must* route to the IP you just assigned to your t0 uplink interface.
 

####<a id='create-objects-logical-router-pks-mgmt'></a> Create T1 Logical Router for PKS Management VMs

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-A6042263-374F-4292-892E-BC86876325A4.html) a Tier-1 (T1) logical router for PKS management VMs named `t1-pks-mgmt`:
  * Link to the `t0-pks` logical router you created in a previous step.
  * Select `edge-cluster-pks` for the cluster.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) a logical router port for `ls-pks-mgmt` and assign the following CIDR block: `172.31.0.1/24`.
1. [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-B6AF9E08-1334-4D3A-B8ED-D0CAB3B563FB.html) route advertisement on the T1 as follows:
   * Enable **Status**.
   * Enable **Advertise All NSX Connected Routes**.
   * Enable **Advertise All NAT Routes**.
   * Enable **Advertise All LB VIP Routes**.
1. Create an SNAT rule on the T1 to allow the PKS Management VMs to communicate with your vCenter and NSX Manager environments. Limit the Destination CIDR for the SNAT rules to the subnet(s) that contain your vCenter and NSX Manager IP addresses. Use the following settings:<br><br>
  * **Opsman & BOSH Director -> DNS**
  * **Opsman & BOSH Director -> NTP**
  * **Opsman & BOSH Director -> vCenter**
  * **Opsman & BOSH Director -> ESXi**
  * **Opsman & BOSH Director -> NSX-T Manager**

    For example, an SNAT rule that maps `172.31.0.0/24` to `10.172.1.1`, where `10.172.1.1` is a routable IP from your **PKS MANAGEMENT CIDR**. For more information, see [Configure Source NAT on a Tier-1 Router](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-31CEF010-0C34-4C10-9443-13A0EAAABFD6.html) in the VMware documentation.

1. Create a DNAT rule on the T1 to map an external IP from the **PKS MANAGEMENT CIDR** to the IP where you will deploy Ops Manager on the `ls-pks-mgmt` logical switch. Use the following settings:<br><br>
  * **External -> Opsman**
  * **External -> Pivotal Container Service**

    For example, a DNAT rule that maps `10.172.1.2` to `172.31.0.2`, where `172.31.0.2` is the IP address you assign to Ops Manager when connected to `ls-pks-mgmt`. For more information, see [Configure Destination NAT on a Tier-1 Router](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-0B572AF0-8AAE-4C4E-83A6-A7DEF4CF5DCD.html) in the VMware documentation.<br><br>
    Later you will create another DNAT rule to map an external IP from the **PKS MANAGEMENT CIDR** to the PKS endpoint.

####<a id='create-objects-logical-router-pks-service'></a> Create T1 Logical Router for PKS Service VMs

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-A6042263-374F-4292-892E-BC86876325A4.html) a Tier-1 (T1) logical router for PKS Service VMs `t1-pks-service`:
  * Link to the `t0-pks` logical switch you created in a previous step.
  * Select `edge-cluster-pks` for the cluster.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) a logical router port for `ls-pks-service` and assign the following CIDR block: `172.31.2.1/23`.
* [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-B6AF9E08-1334-4D3A-B8ED-D0CAB3B563FB.html) route advertisement on the T1 as follows:
   * Enable **Advertise All NSX Connected Routes**.
   * Enable **Advertise All NAT Routes**.
   * Enable **Advertise All LB VIP Routes**.
* [Create](https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-31CEF010-0C34-4C10-9443-13A0EAAABFD6.html) an SNAT rule on the T1 to allow the Kubernetes Cluster VMs to communicate with your environment's NSX Manager and allow the NCP pod on each cluster to communicate with your NSX Manager. Use the following settings:<br><br>
  * **K8s Workers -> External Registries** (example: DockerHub)
  * **K8s Workers -> DNS**
  * **K8s Workers -> NTP**
  * **K8s Workers -> NSX-T Manager** (NCP)
  * **K8s Workers -> vCenter** (vSphere Cloud Provider)
  * **K8s Workers -> External Service Endpoints for Workloads**

    For example, a SNAT rule that maps `172.31.2.0/23` to `10.172.1.3`, where `10.172.1.3` is a routable IP from your **PKS MANAGEMENT CIDR**.
    <p class="note"><strong>Note</strong>: Limit the Destination CIDR for the SNAT rules to the subnet(s) that contain your vCenter and NSX Manager IP addresses.</p>


##<a id='deploy-ops-man'></a> Step 4: Deploy Ops Manager

Complete the procedures in [Deploying Ops Manager to vSphere](vsphere-om-deploy.html).

##<a id='config-ops-man'></a> Step 5: Configure Ops Manager

Perform the following steps to configure Ops Manager for the NSX logical switches:

1. Complete the procedures in [Configuring Ops Manager on vSphere](vsphere-om-config.html).
  <p class="note"><strong>Note</strong>: If you have Pivotal Application Service (PAS) installed, Pivotal recommends installing PKS on a separate instance of Ops Manager v2.0.</p>
  * On the **vCenter Config** page, select **Standard vCenter Networking**. This configuration is utilized for PAS only. You configure NSX-T integration for PKS in a later step.
1. Click **Create Networks** and create the following two networks:
  * A network for deploying the PKS control plane VM(s) that maps to the NSX logical switch named `ls-pks-mgmt` created for the PKS Management Network in [Step 3: Create Required Objects](#create-objects)
  * A service network for deploying PKS Kubernetes cluster nodes that maps to the NSX logical switch named `ls-pks-service` created for the PKS Service Network in [Step 3: Create Required Objects](#create-objects)
  <p class="note"><strong>Note</strong>: Using this NAT topology, you must have already deployed Ops Manager to the <code>ls-pks-mgmt</code> NSX logical switch by following <a href="#create-objects">Step 3: Create Logical Router for PKS Management VMs</a> above. You will use the DNAT IP address to access Ops Manager.</p>
1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.

##<a id='install'></a> Step 6: Install and Configure PKS

Perform the following steps to install and configure PKS:

1. Perform the procedure in [Step 1: Install PKS](installing-pks-vsphere.html#install) of <em>Installing and Configuring PKS on vSphere</em> to install the PKS tile.
1. Click the orange **Pivotal Container Service** tile to start the configuration process.
    <p class="note"><strong>Note</strong>: Configuration of NSX-T or Flannel <strong>cannot</strong> be changed after initial installation and configuration of PKS.</p>

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.
1. Select an availability zone for your singleton jobs and one or more availability zones to balance other jobs in.
  <p class="note"><strong>Note</strong>: In PKS, Pivotal Container Service is a singleton job. This broker VM enables the creation of PKS clusters through the PKS CLI.</p>
1. Under **Network**, select the PKS Management Network linked to the `ls-pks-mgmt` NSX logical switch you created in [Step 5: Configure Ops Manager](#config-ops-man). This will provide network placement for the PKS broker. 
1. Under **Service Network**, select the PKS Service Network linked to the `ls-pks-service` NSX logical switch you created in [Step 5: Configure Ops Manager](#config-ops-man). This will provide network placement for the on-demand Kubernetes cluster service instances created by the PKS broker.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the procedure in the [PKS API](installing-pks-vsphere.html#pks-api) section of <em>Installing and Configuring PKS on vSphere</em>.

###<a id='plan'></a> Plans

Perform the procedures in the [Plans](installing-pks-vsphere.html#plans) section of <em>Installing and Configuring PKS on vSphere</em>.

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

Perform the procedures in the [Kubernetes Cloud Provider](installing-pks-vsphere.html#cloud-provider) section of <em>Installing and Configuring PKS on vSphere</em>.

###<a id='networking'></a> Networking

Perform the following steps:

1. Click **Networking**.
1. Under **Network**, select **NSX-T** as the **Container Network Type** to use.
  ![NSX-T Networking configuration pane in Ops Manager](images/network-nsx-t.png)
1. For **NSX Manager hostname**, enter the NSX Manager hostname or IP address.
1. For **NSX Manager credentials**, enter the credentials to connect to the NSX Manager.
1. For **NSX Manager CA Cert**, optionally enter the custom CA certificate to be used to connect to the NSX Manager.
1. The **Disable SSL certificate verification?** checkbox is **not** selected by default. In order to disable TLS verification, select the checkbox. You may want to disable TLS verification if you did not enter a CA certificate, or if your CA certificate is self-signed.
1. For **vSphere Cluster Name**, enter the name of the vSphere cluster you used when creating the PKS broker in [Assign AZs and Networks](#azs-networks).
1. For **T0 Router ID**, enter the `t0-pks` T0 router UUID. This can be located in the NSX-T UI router overview.
1. For **IP Block ID**, enter the `ip-block-pks-deployments` IP block UUID. This can also be located in the NSX-T UI.
1. For **Floating IP pool ID**, enter the `ip-pool-vips` Floating IP pool ID that was created for load balancer VIPs.
1. Click **Save**.

###<a id='uaa'></a> UAA

Perform the procedures in the [UAA](installing-pks-vsphere.html#uaa) section of <em>Installing and Configuring PKS on vSphere</em>.

###<a id='syslog'></a> (Optional) Syslog

Perform the procedures in the [Syslog](installing-pks-vsphere.html#syslog) section of <em>Installing and Configuring PKS on vSphere</em>.
<p class="note"><strong>Note</strong>: BOSH Director logs contain sensitive information that should be considered privileged.
For example, these logs may contain cloud provider credentials.
If you choose to forward logs to an external syslog endpoint, using TLS encryption is strongly recommended to prevent information from being intercepted by a third party.</p>

###<a id='errands'></a> Errands

<p class="note warning"><strong>WARNING</strong>: You must enable the NSX-T Validation errand in order to verify and tag required NSX-T objects.</p>

Perform the following steps:

1. Click **Errands**.
1. For **Post Deploy Errands**, select **ON** for the **NSX-T Validation errand**. This errand will validate your NSX-T configuration and will tag the proper resources.
1. Click **Save**.

###<a id='resources'></a> (Optional) Resource Config and Stemcell

To modify the resource usage or stemcell configuration of PKS, see [(Optional) Resource Config](installing-pks-vsphere.html#resource-config) and [(Optional) Stemcell](installing-pks-vsphere.html#stemcell) in <em>Installing and Configuring PKS on vSphere</em>.

##<a id='apply-changes'></a> Step 7: Apply Changes and Retrieve the PKS Endpoint

1. After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the PKS tile.
1. When the installation is completed, retrieve the PKS endpoint by performing the following steps:
  1. From the Ops Manager Installation Dashboard, click the **Pivotal Container Service** tile.
  1. Click the **Status** tab and record the IP address assigned to the `Pivotal Container Service` job.
1. Create a DNAT rule on the `t1-pks-mgmt` T1 to map an external IP from the **PKS MANAGEMENT CIDR** to the PKS endpoint. For example, a DNAT rule that maps `10.172.1.4` to `172.31.0.4`, where `172.31.0.4` is PKS endpoint IP address on the `ls-pks-mgmt` NSX Logical Switch. For more information, see [Configure Destination NAT on a Tier-1 Router]( https://docs.vmware.com/en/VMware-NSX-T/2.1/com.vmware.nsxt.admin.doc/GUID-0B572AF0-8AAE-4C4E-83A6-A7DEF4CF5DCD.html) in the VMware documentation.
  <p class="note"><strong>Note</strong>: Ensure that you have no overlapping NAT rules. If your NAT rules overlap, you cannot reach Ops Manager from VMs in the vCenter network.</p>

Developers should use the DNAT IP address when logging in with the PKS CLI. For more information, see [Using PKS](using.html).

<p class="note warning"><strong>WARNING</strong>: The PKS CLI is under active development and commands may change. To ensure you have installed the latest version, we recommend that you re-install the PKS CLI before you use it. See <a href="installing-pks-cli.html">Installing the PKS CLI</a>.</p>

##<a id='nsxt-master-nat'></a> Step 8: Deploy a Cluster and Enable NAT Access

In the current version of PKS, NSX-T does not automatically configure a NAT for the master node of each Kubernetes cluster. As a result, you must perform the following procedure for each cluster to enable your developers to use `kubectl`: 

1. Download the NSX scripts:
  <pre class="terminal">
  $ wget https&#58;//storage.googleapis.com/pks-releases/nsx-helper-pkg.tar.gz
  </pre>
1. Untar the `nsx-helper-pkg.tar.gz` file:
  <pre class="terminal">
  $ tar -xvzf nsx-helper-pkg.tar.gz
  </pre>
1. Install required packages:
  <pre class="terminal">
  $ sudo apt-get install git
  $ sudo apt-get install -y httpie 
  $ sudo apt-get install jq
  </pre>
1. One of the files from the tarball is `nsx-cli.sh`. Make the script executable:
  <pre class="terminal">
  $ chmod 755 nsx-cli.sh
  </pre>
1. Set your NSX Manager admin user, password, and IP address as environment variables named `NSX_MANAGER_USERNAME`, `NSX_MANAGER_PASSWORD`, and `NSX_MANAGER_IP`. For example:
  <pre class="terminal">
  $ export NSX&#95;MANAGER&#95;USERNAME="admin-user" 
  $ export NSX&#95;MANAGER&#95;PASSWORD="admin-password"
  $ export NSX&#95;MANAGER&#95;IP="192.0.2.1" 
  </pre>
1. Execute the `nsx-cli` script with the following command:
  <pre class="terminal">
  $ ./nsx-cli.sh ipam allocate
  </pre>
  Developers can use this IP address as the `--external-hostname` value to create a cluster via the PKS CLI. For more information, see [Using PKS](using.html).
1. Collect the Cluster UUID after cluster has been successfully created. 
   <pre class="terminal">
   $ pks clusters
   </pre>
1. Use the `nsx-cli` script to create a NAT rule to allow access to the Kubernetes API for the cluster. Execute the following command:
   <pre class="terminal">
   $ ./nsx-cli.sh nat create-rule CLUSTER-UUID MASTER-IP NAT-IP
   </pre>
    <br>
    Where:
    <br>
    * `CLUSTER-UUID` is the ID of the cluster retrieved in the previous step.
    * `MASTER-IP` is the IP address that BOSH has assigned to the master node of the cluster. To retrieve this value, use the [BOSH CLI v2](https://bosh.io/docs/cli-v2.html) to log in to your BOSH Director and list all instances with `bosh -e YOUR-ENV instances`.
    * `NAT-IP` is the NAT IP from the `ip-pool-vips` NSX IP pool retrieved above.

##<a id='nsxt-cleanup'></a> Step 9: Clean NSX-T Objects After Deletion of a Cluster

In the current version of PKS, NSX-T does not automatically delete NSX-T objects created during the life of the product.  After a cluster is deleted, you **must** perform the following task using the `nsx-cli.sh` script downloaded in [Step 7](installing-nsx-t.html#nsxt-master-nat):

1. [Delete](delete-cluster.html) the Kubernetes Cluster using the PKS CLI. 

2. Execute the `nsx-cli` script with the following command:
   <pre class="terminal">
   $ ./nsx-cli.sh cleanup CLUSTER-UUID false
   </pre>
    Where `CLUSTER-UUID` is the ID of the cluster you deleted.

