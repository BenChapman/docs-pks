---
title: Installing and Configuring PKS with NSX-T Integration
owner: PKS
---

<p class="note"><strong>Note</strong>: The PKS documentation is under development. This topic will continue to be updated and expanded to reflect the most current information.</p>

This topic describes how to install and configure Pivotal Container Service (PKS) on vSphere with NSX-T integration.

Before performing the procedures in this topic, consult the requirements in the [PKS With NSX-T Integration](requirements.html#pks-with-nsx-t) section of <em>Prerequisites and Resource Requirements</em>.

##<a id='overview'></a> Overview

<p class="note"><strong>Note</strong>:The following diagram details the architecture of a NSX-T with PKS Network Address Translation (NAT) deployment described in this topic.</p> 


![NSX & PKS Overview](images/nsx-t-overview.png)

Click [here](https://docs-pks.cfapps.io/pks/images/nsx-t-overview.png) to view a larger version of this image.

This NAT Topology as depicted in the image above, has the following characteristcs:

* BOSH Director, Operations Manager, and PKS Service Instance are all located on a Logical Switch NAT'd behind a T1.
* All Kubernetes Cluster Nodes are all located on a Logical Switch NAT'd behind a T1.   This will require NAT rules to allow acces to Kubernetes APIs.

Refer to the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) to create/assign the following network CIDRs in the IPv4 address space and ensure they are routable in your environment:

* **VTEP CIDR(s)**: This network(s) will host your GENEVE Tunnel Endpoints on your NSX Transport Nodes. Size the network(s) to support all of your expected Host and Edge Transport Nodes. For example, a CIDR of `192.168.1.0/24` will provide 254 usable IPs.
* **PKS MANAGEMENT CIDR**: This small network will be used for NAT access to PKS management components like Ops Manager. For example, a CIDR of `10.172.1.0/28` will provide 14 usable IPs.
* **PKS LB CIDR**: This network will provide your load balancing address space for each Kubernetes cluster created by PKS. For example, `10.172.2.0/25` will provide 126 usable IPs.

You must also refer to the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) in order to provide or configure your network topology to enable the following:

* vCenter, NSX-T components, and ESXi hosts must be able to communicate with each other.
* The Ops Manager Director VM must be able to communicate with vCenter and the NSX-T Manager.
* The Ops Manager Director VM must be able to communicate with all nodes in all Kubernetes clusters.
* Each Kubernetes cluster deployed by PKS will deploy a NCP pod that must be able to communicate with the NSX-T Manager.

##<a id='deploy-nsx-t'></a> Step 1: Deploy NSX-T

Refer to the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) in order to deploy the following core NSX-T components:


1. Deploy the NSX-T Manager.
1. Deploy the NSX-T Controller(s).
1. Join the NSX-T Control Cluster to the NSX-T Manager and initialize the NSX Controller Cluster.
1. Add your ESX host(s) to the NSX-T Fabric.
  * Each Host must have at least one free vmnic not already used by other vSwitches on the ESX host for use with Host Transport Nodes.
1. [Deploy](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-11417AA2-5EBC-49C7-8A86-EB94604261A6.html) NSX Edge VMs(s). When deploying NSX Edge VMs, you must connect the vNICs of the NSX Edge VMs to an appropriate PortGroup for your environment by performing the following steps:
  1. Connect the first Edge interface to your environment's PortGroup/VLAN where your Edge Management IP can route and communicate with the NSX-T Manager.
  1. Connect the second Edge interface to your environment's PortGroup/VLAN where your T0 uplink interface will be located.  Your **PKS MANAGEMENT CIDR** and **PKS LB CIDR** should be routable to this PortGroup.
  1. Connect the third Edge interface to your environment's PortGroup/VLAN where your GENEVE VTEPs can route and communicate with each other. Your **VTEP CIDR** should be routable to this PortGroup.
  1. [Join](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-11BB4CF9-BC1D-4A76-A32A-AD4C98CBF25B.html) the NSX Edge VM(s) to the NSX-T Fabric.

<p class="note"><strong>Note</strong>: Each NSX Edge VM deployed will require free resources in your vSphere Environment to provide 8 vCPU, 16 GB of RAM, and 120 GB of storage.</p>

##<a id='create-objects'></a> Step 2: Create Required NSX-T Objects for PKS

The following sections refer to topics in the [NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T/index.html) to describe how to create the objects in the NSX-T deployment required for PKS. 

###<a id='create-objects-network-objects'></a> Create NSX Network Objects

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-E7F7322D-D09B-481A-BD56-F1270D7C9692.html) two NSX IP pools:
  * One NSX IP pool for GENEVE Tunnel Endpoints `ip-pool-vteps,` within the usable range of the **VTEP CIDR** created in the previous section
  * One NSX IP pool for NSX Load Balancing VIPs `ip-pool-vips,` within the usable range of the **PKS LB CIDR** created in the previous section
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-F739DC79-4358-49F4-9C58-812475F33A66.html) two NSX Transport Zones (TZs):
  * One NSX TZ for PKS control plane Services and Kubernetes Cluster deployment overlay network(s) called `tz-overlay` and the associated host switch `hs-overlay`
  * One NSX TZ for NSX Edge uplinks (ingress/egress) for PKS Kubernetes cluster(s) called `tz-vlan` and the associated host switch `hs-vlan`
1. If the default uplink profile is not applicable in your use case, [create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-50FDFDFB-F660-4269-9503-39AE2BBA95B4.html) a NSX uplink host profile.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-D7CA778B-6554-4A23-879D-4BC336E01031.html) NSX Host Transport Node(s) 
  * For each host in the NSX-T Fabric, create a node named `tnode-host-NUMBER`. For example, if you have three hosts in the NSX-T Fabric, create three nodes named `tnode-host-1`, `tnode-host-2`, and `tnode-host-3`.
  * Add the `tz-overlay` NSX Transport Zone to each NSX Host Transport Node.
  <p class="note"><strong>Note</strong>: The Transport Nodes must be placed on free host NICs not already used by other vSwitches on the ESX host. Use the `ip-pool-vteps` IP pool that will allow them to route and communicate with each other as well as other Edge Transport Nodes to build GENEVE tunnels.</p>

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-46C7B20D-4BE4-400E-AF39-1ADFE945DE38.html) a NSX IP Block named `ip-block-pks-deployments`. The NSX-T Container Plug-in (NCP) and PKS will use this IP Block to assign address space to Kubernetes pods through the Container Networking Interface (CNI). Pivotal recommends using the CIDR block `172.16.0.0/16`.

###<a id='create-objects-logical-switches'></a> Create Logical Switches

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-23194F9A-416A-40EA-B9F7-346B391C3EF8.html) the following NSX Logical Switches:
  * One for T0 ingress/egress uplink port `ls-pks-uplink` 
  * One for the PKS Management Network `ls-pks-mgmt`
  * One for the PKS Service Network `ls-pks-service`
1. Attach your first NSX Logical Switch to the `tz-vlan` NSX Transport Zone.
1. Attach your second and third NSX Logical Switches to the `tz-overlay` NSX Transport Zone.

###<a id='create-objects-create-nsx-edge'></a> Create NSX Edge Objects


1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-53295329-F02F-44D7-A6E0-2E3A9FAE6CF9.html) NSX Edge Transport Node(s).
1. Add both `tz-vlan` and `tz-overlay` NSX Transport Zones to the NSX Edge Transport Node(s).
1. Refer to the MAC addresses of the Edge VM interfaces you deployed to deploy your virtual NSX Edge(s):
  1. Connect the `hs-vlan` host switch to the vNIC (`fp-eth#`) that matches the MAC address of the second NIC from your deployed Edge VM.
  1. Connect the `hs-overlay` host switch to the vNIC (`fp-eth#`) that matches the MAC address of the third NIC from your deployed Edge VM. 
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.install.doc/GUID-898099FC-4ED2-4553-809D-B81B494B67E7.html) an NSX Edge cluster called `edge-cluster-pks`.
1. Add the NSX Edge Transport Node(s) to the cluster.

###<a id='create-objects-logical-routers'></a> Create Logical Routers

####<a id='create-objects-logical-router-pks'></a> Create Logical Router for PKS

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-7891E6E7-606D-4F79-8AB7-BC01898F9FE7.html) a Tier-0 (T0) logical router named `t0-pks`: 
  * Select `edge-cluster-pks` for the cluster.   
  * Set **High Availability Mode** to **Active-Standby**. 
1. [Attach](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html#GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4) the logical router to the `ls-pks-uplink` logical switch you created in a previous step.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) a logical router port for `ls-pks-uplink` and assign an IP address and CIDR that your environment will use to route to all PKS assigned IP pools and IP blocks.
1. [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-3F163DEE-1EE6-4D80-BEBF-8D109FDB577C.html) T0 routing to the rest of your environment using the appropriate routing protocol for your environment or by using static routes. Keep in mind the following:
  *  The CIDR used in `ip-pool-vips` must route to this IP.


####<a id='create-objects-logical-router-pks-mgmt'></a> Create Logical Router for PKS Management VMs

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-A6042263-374F-4292-892E-BC86876325A4.html) a Tier-1 (T1) logical router for PKS management VMs named `t1-pks-mgmt`:
  * Link to the `t0-pks` logical router you created in a previous step.
  * Select `edge-cluster-pks` for the cluster.
* [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) a logical router port for `ls-pks-mgmt` and assign the following CIDR block: `172.31.0.1/24`.
* [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-B6AF9E08-1334-4D3A-B8ED-D0CAB3B563FB.html) route advertisement on the T1 as follows:
   * Enable **Status**.
   * Enable **Advertise All NSX Connected Routes**.
   * Enable **Advertise All NAT Routes**.
   * Enable **Advertise All LB VIP Routes**.
* [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-31CEF010-0C34-4C10-9443-13A0EAAABFD6.html) a SNAT rule on the T1 to allow the Management VMs to communicate with your environment. For example, a SNAT rule that maps `172.31.0.0/24` to `10.172.1.1`, where `10.172.1.1` is a routable IP from your **PKS MANAGEMENT CIDR**.
* [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-0B572AF0-8AAE-4C4E-83A6-A7DEF4CF5DCD.html) a DNAT rule on the T1 to map an external IP from the **PKS MANAGEMENT CIDR** to the IP where you will deploy Ops Manager on the `ls-pks-mgmt` logical switch. For example, a DNAT rule that maps `10.172.1.2` to `172.31.0.2`, where `172.31.0.2` is the IP address you assign to Ops Manager when connected to `ls-pks-mgmt`.

####<a id='create-objects-logical-router-pks-service'></a> Create Logical Router for PKS Service VMs

1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-A6042263-374F-4292-892E-BC86876325A4.html) a Tier-1 (T1) Logical Router for PKS Service VMs `t1-pks-service`:
  * Link to the `t0-pks` logical switch you created in a previous step.
  * Select `edge-cluster-pks` for the cluster.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-D641380B-4C8E-4C8A-AF64-4261A266ACA4.html) a logical router port for `ls-pks-service` and assign the following CIDR block: `172.31.2.1/23`.
* [Configure](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-B6AF9E08-1334-4D3A-B8ED-D0CAB3B563FB.html) route advertisement on the T1 as follows:
   * Enable **Advertise All NSX Connected Routes**.
   * Enable **Advertise All NAT Routes**.
   * Enable **Advertise All LB VIP Routes**.
* [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-31CEF010-0C34-4C10-9443-13A0EAAABFD6.html) a SNAT rule on the T1 to allow the Kubernetes Cluster VMs to communicate with your environment's NSX-T Manager. For example, a SNAT rule that maps `172.31.2.0/23` to `10.172.1.1`, where `10.172.1.1` is a routable IP from your **PKS MANAGEMENT CIDR**.

##<a id='config-ops-man'></a> Step 3: Configure Ops Manager

Perform the following steps to configure Ops Manager for the NSX logical switches:

<p class="note"><strong>Note</strong>: Using this NAT topology, you must have already deployed Ops Manager to the <code>ls-pks-mgmt</code> NSX logical switch with an IP that successfully resolves through NAT, such as <code>172.31.0.2/24</code>. Your <code>YOUR-OPSMAN-FQDN</code> should resolve to this IP.</p>

1. Navigate to `YOUR-OPSMAN-FQDN` in a browser to log in to the Ops Manager Installation Dashboard.
1. Click the **Ops Manager Director** tile.
1. Click **Create Networks**.
1. Create the following two networks:
  * A network for deploying the PKS control plane VM(s) that maps to the NSX logical switch named `ls-pks-mgmt` created for the PKS Management Network in [Step 3: Create Required Objects](#create-objects)
  * A service network for deploying PKS Kubernetes cluster nodes that maps to the NSX logical switch named `ls-pks-service` created for the PKS Service Network in [Step 3: Create Required Objects](#create-objects)
  <p class="note"><strong>Note</strong>: Do not enable <strong>NSX Networking</strong> on the <strong>vCenter Config</strong> page. You will configure NSX-T integration for PKS in a following step.</p>
1. Return to the Ops Manager Installation Dashboard and click **Apply Changes**.


##<a id='install'></a> Step 4: Install and Configure PKS

Perform the following steps to install and configure PKS:

1. Perform the procedure in [Step 1: Install PKS](installing.html#install) of <em>Installing and Configuring PKS</em> to install the PKS tile.
1. Click the orange **Pivotal Container Service** tile to start the configuration process.
    <p class="note"><strong>Note</strong>: Configuration of NSX-T or Flannel <strong>cannot</strong> be changed after initial installation and configuration of PKS.</p>

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.
1. Select an availability zone for your singleton jobs, and one or more availability zones to balance other jobs in. This is where PCF creates the PKS broker.
1. Under **Network**, select the PKS Management Network linked to the `ls-pks-mgmt` NSX logical switch you created in [Step 4: Configure Ops Manager](#config-ops-man). This will provide network placement for the PKS broker. 
1. Under **Service Network**, select the PKS Service Network linked to the `ls-pks-service` NSX logical switch you created in [Step 4: Configure Ops Manager](#config-ops-man). This will provide network placement for the on-demand Kubernetes cluster service instances created by the PKS broker.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the procedure in [PKS API](installing.html#pks-api) of <em>Installing and Configuring PKS</em>.

###<a id='broker'></a> Broker

Perform the procedure in [Broker](installing.html#broker) of <em>Installing and Configuring PKS</em>.

###<a id='plan'></a> Plan

Perform the procedures in [Plan](installing.html#plan) of <em>Installing and Configuring PKS</em>.

###<a id='iaas'></a> IaaS

Perform the procedures in [IaaS](installing.html#iaas) of <em>Installing and Configuring PKS</em>.

###<a id='networking'></a> Networking

Perform the following steps:

1. Click **Networking**.
1. Under **Network**, select **NSX-T** as the **Container Network Type** to use.
1. For **Cluster Name**, enter the name of the vSphere cluster you used when creating the PKS broker in [Assign AZs and Networks](#azs-networks).
1. For **NSX Manager hostname**, enter the NSX Manager hostname or IP address.
1. For **NSX Manager credentials**, enter the credentials to connect to the NSX Manager.
1. For **NSX Manager CA Cert**, optionally enter the custom CA certificate to be used to connect to the NSX Manager.
1. The **Disable SSL certificate verification?** checkbox is **not** selected by default. In order to disable TLS verification, select the checkbox. You may want to disable TLS verification if you did not enter a CA certificate, or if your CA certificate is self-signed.
1. For **Tier 0 Router ID**, enter the `t0-pks` T0 router ID. This can be located in the NSX-T UI router overview.
1. For **IP Block ID**, enter the `ip-block-pks-deployments` IP block ID. This can also be located in the NSX-T UI.
1. For **Floating IP pool ID**, enter the `ip-pool-vips` Floating IP pool ID that was created for load balancer VIPs.
1. Click **Save**.

###<a id='errands'></a> Errands

Perform the following steps:

1. Click **Errands**.
1. For **Post Deploy Errands**, select **Default (On)** for the **NSX-T Validation errand**. This errand will validate your NSX-T configuration and will tag the proper resources.
1. Click **Save**.

###<a id='resources'></a> (Optional) Resource Config and Stemcell

To modify the resource usage or stemcell configuration of PKS, see [(Optional) Resource Config](installing.html#resource-config) and [(Optional) Stemcell](installing.html#stemcell) in <em>Installing and Configuring PKS</em>.

##<a id='apply-changes'></a> Step 5: Apply Changes and Retrieve the PKS Endpoint

1. After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the PKS tile.
1. When the installation is completed, retrieve the PKS endpoint by performing the following steps:
  1. From the Ops Manager Installation Dashboard, click the **Pivotal Container Service** tile.
  1. Click the **Status** tab and record the IP address assigned to the `Pivotal Container Service` job.
1. [Create](https://docs.vmware.com/en/VMware-NSX-T/2.0/com.vmware.nsxt.admin.doc/GUID-0B572AF0-8AAE-4C4E-83A6-A7DEF4CF5DCD.html) a DNAT rule on the `t1-pks-mgmt` T1 to map an external IP from the **PKS MANAGEMENT CIDR** to the PKS endpoint. For example, a DNAT rule that maps `10.172.1.2` to `172.31.0.4`, where `172.31.0.4` is PKS endpoint.

Developers should use the DNAT IP address when logging in with the PKS CLI. For more information, see [Using PKS](using.html).

##<a id='nsxt-master-nat'></a> Step 6: Enable NAT Access

In the current version of PKS, NSX-T does not automatically configure a NAT for the master node of each Kubernetes cluster. As a result, you must perform the following procedure for each cluster to enable your developers to use `kubectl`: 

1. [Download](https://www.python.org/downloads/) and install Python v3.x. 
1. Download and install the Python package manager:
  <pre class="terminal">
  $ curl --silent http<span>s:</span>//bootstrap.pypa.io/get-pip.py | sudo python3
  </pre>
1. Download the NSX scripts:
  <pre class="terminal">
  $ wget http<span>s:</span>//storage.googleapis.com/pks-releases/nsx_cleanup.tar.gz
  </pre>
1. Untar the `nsx_cleanup.tar.gz` file:
  <pre class="terminal">
  $ tar xvf nsx_cleanup.tar.gz
  </pre>
1. One of the files from the tarball is `nsx-cli.sh`. Make the script executable:
  <pre class="terminal">
  $ chmod 755 nsx-cli.sh
  </pre>
1. Set your NSX Manager admin user, password, and IP address as environment variables named `NSX_MANAGER_USERNAME`, `NSX_MANAGER_PASSWORD`, and `NSX_MANAGER_IP`. For example:
  <pre class="terminal">
  $ export NSX_MANAGER_USERNAME="admin-user" 
  $ export NSX_MANAGER_PASSWORD="admin-password"
  $ export NSX_MANAGER_IP="192.0.2.1" 
1. Log in to the NSX-T UI and retrieve the ID of the `ip-pool-vips` NSX IP pool.
1. Execute the `nsx-cli` script with the following command, specifying the ID of `ip-pool-vips` as `IP-POOL-ID`:

    ```
    ./nsx-cli.sh ipam allocate IP-POOL-ID
    ```
1. In the NSX-T UI, find an available NAT IP from the `ip-pool-vips` NSX IP pool. Developers can use this IP address as the `--external-hostname` value to create a cluster via the PKS CLI. For more information, see [Using PKS](using.html).
1. Use the `nsx-cli` script to create a NAT rule to allow access to the Kubernetes API for the cluster. Execute the following command:

    ```
    ./nsx-cli.sh nat create-rule T0-ROUTER-ID NAT-IP MASTER-IP
    ```
    <br>
    Where:
    <br>
    * `T0-ROUTER-ID` is the ID of the `t0-pks` T0 router. Retrieve this value from the NSX-T UI.
    * `NAT-IP` is the NAT IP from the `ip-pool-vips` NSX IP pool retrieved above.
    * `MASTER-IP` is the IP address that BOSH has assigned to the master node of the cluster. To retrieve this value, use the [BOSH CLI v2](https://bosh.io/docs/cli-v2.html) to log in to your BOSH Director and list all instances with `bosh -e YOUR-ENV instances`.