---
title: Maintain Workload Uptime
owner: PKS
---

This topic describes how you can maintain workload uptime for Kubernetes clusters deployed with Pivotal Container Service (PKS).

To maintain workload uptime, configure the following settings in your deployment manifest:

1. Configure [workload replicas](#replicas) to handle traffic during rolling upgrades.
1. Define an [anti-affinity rule](#anti-affinity) to evenly distribute workloads across the cluster.

To increase uptime, you can also refer to the documentation for the services you run on your clusters and configure your workload based on the best practices recommended by the software vendor.

##<a id='upgrades'></a> About Workload Upgrades

The PKS tile contains an errand that upgrades all Kubernetes clusters.
Upgrades run on a single VM at a time.
While one worker VM runs an upgrade, the workload on that VM goes down.
The additional worker VMs continue to run replicas of your workload, maintaining the uptime of your workload.

To prevent workload downtime during a cluster upgrade, Pivotal recommends running your workload on at least three worker VMs, using multiple replicas of your workloads spread across those VMs.
You must edit your manifest to define the replica set and configure an anti-affinity rule to ensure that the replicas run on separate worker nodes.

##<a id='replicas'></a> Set Workload Replicas

Set the number of workload replicas to handle traffic during rolling upgrades.
To replicate your workload on additional worker VMs, deploy the workload using a replica set.

Edit the `spec.replicas` value in your deployment manifest:

```yaml
kind: Deployment
metadata:
  # ...
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: APP-NAME
```

See the following table for more information about this section of the manifest:

<table>
  <tr>
    <th>Key-Value Pair</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><pre>spec:<br>  replicas: 3</pre></td>
    <td> Set this value to at least 3 to have at least three instances of your workload running at any time.
   </td>
  </tr>
  <tr>
    <td><pre>app: APP-NAME</pre></td>
    <td>Use this app name when you define the anti-affinity rule later in the spec.</td>
  </tr>
</table>

##<a id='anti-affinity'></a> Define an Anti-Affinity Rule

To distribute your workload across multiple worker VMs, you must use anti-affinity rules.
If you do not define an anti-affinity rule, the replicated pods can be assigned to the same worker node.
See the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) for more information about anti-affinity rules.

To define an anti-affinity rule, add the `spec.template.spec.affinity` section to your deployment manifest:


```yaml
kind: Deployment
metadata:
  # ...
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: APP-NAME
    spec:
      containers:
      - name: MY-APP
        image: MY-IMAGE
        ports:
        - containerPort: 12345
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - APP-NAME
              topologyKey: "kubernetes.io/hostname"
```

See the following table for more information:

<table>
  <tr>
    <th>Key-Value Pair</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><pre>matchExpressions: <br>- key: "app"</pre></td>
    <td>This value matches <code>spec.template.metadata.labels.app</code>.</td>
  </tr>
  <tr>
    <td><pre>values: <br>- APP-NAME</pre></td>
    <td>This value matches the <code>APP-NAME</code> you defined earlier in the spec.</td>
  </tr>
</table>


##<a id='multi-az-worker'></a> Multi-AZ Worker

If you would like your workers to be evenly distributed across availability zones (AZs), please configure this option in the PKS tile. Kubernetes will evenly spread pods in a replication controller over multiple zone, this is best-effort placement. For increased control over the placement `Anti-Affinity Rule` need to be added to the deployment spec.

To ensure spreading of pods in the AZs you can replace `"kubernetes.io/hostname"` with `"failure-domain.beta.kubernetes.io/zone"` in the previously show spec snippet.


### Persistent Volumes

Persistent volumes cannot be attached across zones. Therefore, when persistent volumes are created, the PersistentVolumeLabel admission controller automatically adds zone labels to them. The scheduler will then ensure that pods that claim a given volume are only placed into the same zone as that volume.

If an AZ goes down, the persistent volume and its data also goes down and cannot be automatically re-attached. To sustain an AZ failure your persistent workload needs to have failover mechanism in place.

To ensure uptime of persistent workload during cluster upgrade we recommend having at least 2 nodes per AZ. This way Kubernetes will reschedule pods in the other node of the same AZ while BOSH is performing the upgrade.