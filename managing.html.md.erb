---
title: Managing PKS
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to manage Pivotal Container Service (PKS).

Please send any feedback you have to [pks-feedback@pivotal.io](mailto:pks-feedback@pivotal.io).

##<a id='access'></a> Configuring PKS API access

* You must have an external TCP load balancer configured to forward traffic to the PKS API endpoint. For more information, see the [Configure External Load Balancer](installing.html#loadbalancer-pks-api) section of <em>Installing and Configuring PKS</em>.

  <p class="note"><strong>Note</strong> If your PKS installation is integrated with NSX-T, developers should map the external TCP loadbalancer to the DNAT IP address assigned in the <a href="installing-nsx-t.html#apply-changes">Apply Changes and Retrieve the PKS Endpoint</a> section of <em>Installing and Configuring PKS with NSX-T Integration</em>.</p>

* You must configure a DNS entry which points to the TCP load balancer and uses the domain configured in the certificate during [tile configuration](installing.html#pks-api).

##<a id='access'></a> Configuring users in UAA

You configure users in the UAA [using uaac](https://docs.cloudfoundry.org/uaa/uaa-user-management.html). To retrieve the UAA admin client secret, the operator performs the following steps:

1. Navigate to the Pivotal Container Service tile in the Ops Manager Installation Dashboard.
1. Click 'Credentials' tab 
1. Get the 'Uaa Admin Secret'

PKS uses the following scopes:
  * pks.clusters.admin - users with this scope have full access to all clusters.
  * pks.clusters.manage - users with this scope only have access to clusters they created.

```
uaac target https://pks-api.example.com:8443
uaac token client get admin -s <SECRET>
uaac user add alana --emails alana@example.com -p <PASSWORD>
uaac member add pks.clusters.admin alana	
```
	
##<a id='ssh'></a> Manage PKS Deployments with BOSH

To manage your PKS deployment with BOSH, perform the following steps:

1. Gather credential and IP address information for your BOSH Director and SSH into the Ops Manager VM.
See [Advanced Troubleshooting with the BOSH CLI](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html) for more information.
1. Create a BOSH alias for your PKS environment. For example:
  <pre class="terminal">$ bosh alias-env pks -e 10.0.0.3 --ca-cert /var/tempest/workspaces/default/root_ca_certificate</pre>
1. Log in to the BOSH Director.
  <pre class="terminal">$ bosh -e pks log-in</pre>
1. Follow the procedures in the [Use the BOSH CLI for Troubleshooting](https://docs.pivotal.io/pivotalcf/1-12/customizing/trouble-advanced.html#cli) topic to manage your PKS deployment with BOSH.

##<a id='custom-workloads'></a> Add Custom Workloads

To apply custom Kubernetes workloads to every cluster created on a plan, add YAML to the tile config under **Default Cluster Apps**.
Use this configuration to define what a cluster includes out of the box.
For example, use custom workloads to configure metrics or logging.

##<a id='download-logs'></a> Download Cluster Logs

To download cluster logs, perform the following steps:

1. Gather credential and IP address information for your BOSH Director, SSH into the Ops Manager VM, and use the BOSH CLI v2 to log in to the BOSH Director from the Ops Manager VM.
For more information, see [Advanced Troubleshooting with the BOSH CLI](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html).
1. After logging in to the BOSH Director, identify the name of your PKS deployment. For example:
  <pre class="terminal">$ bosh -e pks deployments</pre>
  Your PKS deployment name begins with `pivotal-container-service` and includes a BOSH-generated hash.
1. Identify the names of the VMs you want to retrieve logs from by listing all VMs in your deployment. For example:
	<pre class="terminal">$ bosh -e pks -d pivotal-container-service-aa1234567bc8de9f0a1c vms</pre>
1. Download the logs from the VM. For example:
	<pre class="terminal">$ bosh -e pks -d pivotal-container-service-aa1234567bc8de9f0a1c logs pks/0</pre>
	See [Diagnosing and Troubleshooting PKS](troubleshoot.html#bosh-pks-map) for information about using cluster logs to diagnose issues in your PKS deployment.

##<a id='upgrade-downtime'></a> Prepare Workloads for an Upgrade

To prevent workload downtime during a PKS upgrade, define the following settings in the deployment manifest:

[//]: # (* Set `max_in_flight` to 1.)
[//]: # (This value cannot be configured in the current PKS version.)
* Increase the number of worker nodes by editing the `spec.replicas` value.
* Schedule pod replicas to run on separate workers by defining a `podAntiAffinity` rule.

For example:

```yaml
kind: Deployment
metadata:
  # ...
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: APP-NAME
    spec:
      containers:
      - name: MY-APP
        image: MY-IMAGE
        ports:
        - containerPort: 12345
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - APP-NAME
              topologyKey: "kubernetes.io/hostname"
```

See the following table for descriptions of the values you must edit in the deployment manifest:

<table>
  <tr>
    <th>Key-Value Pair</th>
    <th>Description</th>
  </tr>
  <tr>
    <td><pre>spec:<br>  replicas: 3</pre></td>
    <td>Set this value to at least 2 to increase the number of worker nodes.
    If you are unsure of your worker capacity, begin by increasing the value by 1.</td>
  </tr>
  <tr>
    <td><pre>app: APP-NAME</pre></td>
    <td>Use this app name when you define the anti-affinity rule later in the spec.</td>
  </tr>
  <tr>
    <td><pre>matchExpressions: <br>- key: "app"</pre></td>
    <td>This value matches <code>spec.template.metadata.labels.app</code>.</td>
  </tr>
  <tr>
    <td><pre>values: <br>- APP-NAME</pre></td>
    <td>This value matches the <code>APP-NAME</code> you defined earlier in the spec.</td>
  </tr>
</table>

##<a id='delete-errand'></a> Delete PKS

To delete PKS, perform the following steps:

1. Navigate to the Ops Manager Installation Dashboard.
1. Click the trash icon on the PKS tile.
1. Click **Confirm** in the dialog box that appears.
1. By default, deleting the PKS tile will also delete all the clusters created by PKS. To preserve the clusters, click the **Delete all clusters** errand under **Pending Changes** and select **Off**.
1. Click **Apply Changes**.

