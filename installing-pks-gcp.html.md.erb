---
title: Installing and Configuring PKS on GCP
owner: PKS
---

<strong><%= modified_date %></strong>

This topic describes how to install and configure Pivotal Container Service (PKS) on Google Cloud Platform (GCP).

Before performing the procedures in this topic, see [GCP Prerequisites and Resource Requirements](gcp-requirements.html).

##<a id='loadbalancer-pks-api'></a>Step 1: Configure Load Balancer

Configure an external TCP load balancer to access the PKS API from outside the network.
You can use any external TCP load balancer of your choice.

Refer to the procedures in this section to create a load balancer using GCP.
If you choose to use a different load balancer, use the configuration in this section as a guide.

### <a id='create-lb'></a> Create a Load Balancer Using GCP

To create a load balancer using GCP, perform the following steps:

1. In a browser, navigate to the [GCP console](https://console.cloud.google.com).

1. Navigate to **Network Services > Load balancing** and click **CREATE LOAD BALANCER**.

1. Under **TCP Load Balancing**, click **Start configuration**.

1. Select whether you want to load balance traffic from the Internet to your VMs or only between your VMs.

1. Select whether you want to place the backends for your load balancer in a single region or across multiple regions.

1. Give your load balancer a name. Pivotal recommends naming your load balancer `pks-api`.

1. Select **Backend configuration**.
   * Under **Region**, select the region where you deployed Ops Manager.
   * Under **Backends**, leave **Select existing instance groups** selected. You point the PKS API instance to the backend later in this procedure.
   * (Optional) Select a backup pool.
   * (Optional) Select whether you want to create a health check or go without one.
   * Select a session affinity configuration.
   * (Optional) Select **Advanced configurations** to configure the **Connection draining timeout**.

1. Select **Frontend configuration**.
   * (Optional) Give your frontend a name.
   * (Optional) Give your frontend a description.
   * Select **Create IP address** to reserve an IP address for the PKS API endpoint.
      1. Enter a name for your reserved IP address. For example, `pks-api-ip`. GCP assigns a static IP address that appears next to the name.
      1. (Optional) Enter a description.
      1. Click **Reserve**.
   * Under **Ports**, enter 8443 and 9021. Your external load balancer forwards traffic to the PKS control plane VM using the UAA endpoint on port 8443 and the PKS API endpoint on port 9021.
   * Click **Done**.

1. Click **Review and finalize** to review your load balancer configuration.

1. Click **Create**.

### <a id='firewall-rule'></a> Create a Firewall Rule for the Load Balancer

To create a firewall rule that allows traffic between the load balancer and the PKS API VM, do the following:

1. From the GCP console, navigate to **VPC Network > Firewall rules** and click **CREATE FIREWALL RULE**.

1.  Configure the following:
   * Give your firewall rule a name.
   * (Optional) Give your firewall rule a description.
   * Under **Network**, select the VPC network you created in [Step 3: Create a GCP Network with Subnets](gcp-prepare-env.html#create-network) of _Preparing to Deploy PKS on GCP_.
   * Under **Priority**, enter a priority number between 0 and 65535.
   * Under **Direction of traffic**, select **Ingress**.
   * Under **Action on match**, select **Allow**.
   * Under **Targets**, select the load balancer you created in the previous section.
   * Under **Target tags**, enter `pks-api`.
   * Under **Source filter**, select **IP ranges**.
   * Under **Source IP ranges**, enter `0.0.0.0/0`.
   * Under **Protocols and ports**, select **Specified protocols and ports** and enter `tcp:8443,9021`.

1. Click **Create**.

### <a id='tag'></a> Create a Network Tag for the Firewall Rule

To apply the firewall rule to the VM that hosts the PKS API, the VM must have the `pks-api` tag in GCP. Do the following:

1. From the GCP console, navigate to **Compute Engine** > **VM instances**.
1. Locate the your PKS control plane VM.
1. Click the name of the VM to open the **VM instance details** menu.
1. Click **Edit**.
1. Verify that the **Network tags** field contains the `pks-api` tag. If the tag does not appear in the field, enter it now.
1. Scroll to the bottom of the screen and click **Save**.

### <a id='wildcard-dns-entry'></a> Create a Wildcard DNS Entry for the Load Balancer

To create a wildcard DNS entry in GCP for your PKS API domain, do the following:

1. From the GCP console, navigate to **Network Services > Cloud DNS**.

1. If you do not already have a DNS zone, click **Create zone**.
   * Give a **Zone name** and a **DNS name**.
   * Specify whether the **DNSSEC** state of the zone is **Off**, **On**, or **Transfer**.
   * (Optional) Enter a **Description**.
   * Click **Create**.

1. Click **Add record set**.

1. Under **DNS Name**, enter a subdomain for the load balancer. For example, to use `pks-api.pks.example.com` as your PKS API hostname, enter `pks-api` in this field.

1. Select a **Resource Record Type**, **TTL**, and **TTL Unit**.

1. Enter the static IP address that GCP assigned when you created the load balancer. See the [Create a Load Balancer Using GCP](#create-lb) section for more information.

1. Click **Create**.

##<a id='install'></a> Step 2: Install PKS

To install PKS, do the following:

1. Download the product file from [Pivotal Network](https://network.pivotal.io).
1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.
1. Click the **Google Cloud Platform** tile.
1. Select the **Director Config** tab.
1. Configure the following settings:
  * Select **Enable Post Deploy Scripts**.
  * Clear the **Disable BOSH DNS server for troubleshooting purposes** checkbox.
1. Click **Apply Changes**.
1. Click **Import a Product** to upload the product file.
1. Under **Pivotal Container Service** in the left column, click the plus sign to add this product to your staging area.

##<a id='configure'></a> Step 3: Configure PKS

Click the orange **Pivotal Container Service** tile to start the configuration process.

![Pivotal Container Service tile on the Ops Manager installation dashboard](images/pks-tile-orange.png)

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.

1. Select the availability zone (AZ) where you want to deploy the PKS API VM as a singleton job.
  <p class="note"><strong>Note</strong>: You must select an additional AZ for balancing other jobs before clicking <strong>Save</strong>, but this selection has no effect in the current version of PKS.</p>

    ![Assign AZs and Networks pane in Ops Manager](images/azs-networks.png)

1. Under **Network**, select the infrastructure subnet you created for the PKS API VM.
1. Under **Service Network**, select the services subnet you created for Kubernetes cluster VMs.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the following steps:

1. Click **PKS API**.
1. Under **Certificate to secure the PKS API**, provide your own wildcard certificate and private key pair.
The certificate you enter here should cover the domain that routes to the PKS broker VM with TLS termination on the ingress.<br><br>
    (Optional) If you do not have a certificate and private key pair, you can have Ops Manager generate one for you. Perform the following steps:
  1. Select the **Generate RSA Certificate** link.
  1. Enter the wildcard domain for your API hostname.
    For example, if your PKS API domain is `api.pks.example.com`, then enter `*.pks.example.com`.
  1. Click **Generate**.
1. Click **Save**.

###<a id='plans'></a> Plans

To activate a plan, do the following:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
  <p class="note"><strong>Note</strong>: A plan defines a set of resource types used for deploying clusters. You can configure up to three plans. Configuring **Plan 1** is required.
  </p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.
1. Under **Name**, provide a unique name for the plan.
1. Under **Description**, edit the description as needed.
The plan description appears in the Services Marketplace, which developers can access by using PKS CLI.
1. Under **AZ placement**, select an AZ for the Kubernetes clusters deployed by PKS.
1. Under **Default Cluster Authorization Mode**, select an authentication mode for the Kubernetes clusters. Pivotal recommends selecting **RBAC**. For more information, see the [RBAC Support in Kubernetes](https://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes) blog post.
1. Under **ETCD/Master VM Type**, select the type of VM to use for Kubernetes etcd and master nodes.
1. Under **Master Persistent Disk Type**, select the size of the persistent disk for the Kubernetes master VM.
1. Under **Worker VM Type**, select the type of VM to use for Kubernetes worker nodes.
1. Under **Worker Persistent Disk Type**, select the size of the persistent disk for the Kubernetes worker nodes.
1. Under **Worker Node Instances**, select the default number of Kubernetes worker nodes to provision for each cluster. For high availability, Pivotal recommends creating clusters with at least 3 worker nodes.
1. Under **Errand VM Type**, select the size of the VM where the errand will run. The smallest instance possible is sufficient, as the only errand running on this VM is the one that applies the **Default Cluster App** YAML configuration.
1. (Optional) Under **(Optional) Add-ons - Use with caution**, enter additional YAML configuration to [add custom workloads](custom-workloads.html) to each cluster in this plan. You can specify multiple files using `---` as a separator.
1. If you want users to be able to create pods with privileged containers, select the **Enable Privileged Containers - Use with caution** option. For more information, see [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers) in the Kubernetes documentation.
1. Click **Save**.

To deactivate a plan, do the following:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
1. Select **Plan Inactive**.
1. Click **Save**.

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

To configure your Kubernetes cloud provider settings, do the following:

1. Click **Kubernetes Cloud Provider**.
1. Under **Choose your IaaS**, select **GCP**.
1. Perform the steps specific to GCP.
  * Ensure the values match those in the **Google Config** section of the **Ops Manager** tile:
    1. Enter your **GCP Project Id**, which is the name of the deployment in your Ops Manager environment.
    1. Enter your **VPC Network**, which is the VPC network name for your Ops Manager environment.
    1. Enter your **GCP Master Service Account Key**. For information about configuring this key, see [Create the Master Node Service Account](gcp-prepare-env.html#create-master).
    1. Enter your **GCP Worker Service Account Key**. For information about configuring this key, see [Create the Worker Node Service Account](gcp-prepare-env.html#create-worker).
1. Click **Save**.

###<a id='networking'></a> Networking

To configure networking, do the following:

1. Click **Networking**.
1. Under **Network**, select **Flannel**.
1. Click **Save**.

###<a id='uaa'></a> UAA

To configure the UAA server, do the following:

1. Click **UAA**.
1. Under **UAA URL**, enter a fully qualified domain name (FQDN) to access UAA on the PKS broker VM. This URL must belong to the domain you provided in the [PKS API](#pks-api) section.
For example, if you provided a certificate for `*.pks.example.com`, enter `api.pks.example.com` for the UAA URL.
1. Under **PKS CLI Access Token Lifetime**, enter a time in seconds for the PKS CLI access token lifetime.
1. Under **PKS CLI Refresh Token Lifetime**, enter a time in seconds for the PKS CLI refresh token lifetime.

###<a id='syslog'></a> (Optional) Syslog

You can designate an external syslog endpoint for PKS component and cluster log messages.

To specify the destination for PKS log messages, do the following:

1. Click **Syslog**.
1. Select **Yes** to configure syslog forwarding.
1. Enter the destination syslog endpoint.
1. Enter the destination syslog port.
1. Select a transport protocol for log forwarding.
1. (Optional) Pivotal strongly recommends that you enable TLS encryption when forwarding logs as they may contain sensitive information. For example, these logs may contain cloud provider credentials. To enable TLS, perform the following steps.
  1. Provide the accepted fingerprint (SHA1) or name of remote peer. For example, `*.YOUR-LOGGING-SYSTEM.com`.
  1. Provide a TLS certificate for the destination syslog endpoint.
  <p class="note"><strong>Note</strong>: You do not need to provide a new certificate if the TLS certificate for the destination syslog endpoint is signed by a Certificate Authority (CA) in your BOSH certificate store.</p>

###<a id='errands'></a> Errands

Errands are scripts that run at designated points during an installation. 

To configure when post-deploy and pre-delete errands for PKS are run, make a selection in the dropdown next to the errand. For a typical PKS deployment, Pivotal recommends that you leave the default settings.

For more information about errands and their configuration state, see [Managing Errands in Ops Manager](https://docs.pivotal.io/pivotalcf/customizing/managing_errands.html).

###<a id='resource-config'></a> Resource Config

1. To modify the resource usage of PKS, click **Resource Config** and edit the **Pivotal Container Service** job.
<p class="note"><strong>Note</strong>: If you experience timeouts or slowness when interacting with the PKS API, select a <strong>VM Type</strong> with greater CPU and memory resources for the <strong>Pivotal Container Service</strong> job.</p>

1. Enter a name for your PKS API load balancer that begins with `tcp:` in the **Load Balancers** column.
For example, `tcp:pks-api`.
For more information, see [Step 1: Configure Load Balancer](#loadbalancer-pks-api).

###<a id='stemcell'></a> (Optional) Stemcell

To edit the stemcell configuration, click **Stemcell**. Click **Import Stemcell** to import a new stemcell. 

PKS uses floating stemcells. Floating stemcells allow upgrades to the minor versions of stemcells but not the major versions. 
For example, a stemcell can float from `1234.56` to `1234.99` but not from `1234.991` to `1235.0`. 
For more information on floating stemcells, see [Understanding Floating Stemcells](http://docs.pivotal.io/pivotalcf/customizing/understanding-stemcells.html).

<p class="note warning"><strong>WARNING</strong>: Because PKS uses floating stemcells, updating the PKS tile with a new stemcell triggers the rolling of every VM in each cluster. Also, updating other product tiles in your deployment with a new stemcell causes the PKS tile to roll VMs. This rolling is enabled by the <b>Upgrade all clusters errand</b>. Pivotal recommends that you keep this errand turned on because automatic rolling of VMs ensures that all deployed cluster VMs are patched. However, automatic rolling can cause downtime in your deployment.</p>

##<a id='apply-changes'></a> Step 4: Apply Changes

After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the tile.

##<a id='retrieve-pks-api'></a> Step 5: Retrieve PKS API Endpoint

You must share the PKS API endpoint to allow your organization to use the API to create, update, and delete clusters.
See [Create a Cluster](create-cluster.html) for more information.

To retrieve the PKS API endpoint, do the following:

1. Navigate to the Ops Manager Installation Dashboard. 
1. Click the Pivotal Container Service tile.
1. Click the **Status** tab and locate the **Pivotal Container Service** job. The IP address of the Pivotal Container Service job is the PKS API endpoint.

##<a id='next-steps'></a> Next Steps

Follow the procedures in [Manage Users in UAA](manage-users.html) to configure authentication for PKS using User Account and Authentication (UAA).

After configuring authentication, follow the procedures in [Configure PKS API Access](configure-api.html) to enable operators to create and manage clusters.
