---
title: Installing and Configuring PKS on GCP
owner: PKS
---

<strong><%= modified_date %></strong>

<p class="note"><strong>Note</strong>: The PKS documentation is under development. This topic will continue to be updated and expanded to reflect the most current information.</p>

This topic describes how to install and configure Pivotal Container Service (PKS) on Google Cloud Platform (GCP).

Before performing the procedures in this topic, consult the [GCP Prerequisites and Resource Requirements](gcp-requirements.html) topic.

##<a id='install'></a> Step 1: Install PKS

Perform the following steps to install PKS:

1. Download the product file from [Pivotal Network](https://network.pivotal.io).
1. Navigate to `https://YOUR-OPS-MANAGER-FQDN/` in a browser to log in to the Ops Manager Installation Dashboard.
1. From the **Director Config** page, configure the following settings:
  * Select **Enable Post Deploy Scripts**.
  * Clear the **Disable BOSH DNS server for troubleshooting purposes** checkbox.
1. Click **Apply Changes**.
1. Click **Import a Product** to upload the product file.
1. Under **Pivotal Container Service** in the left column, click the plus sign to add this product to your staging area.

##<a id='configure'></a> Step 2: Configure PKS

Click the orange **Pivotal Container Service** tile to start the configuration process.

![Pivotal Container Service tile on the Ops Manager installation dashboard](images/pks-tile.png)

###<a id='azs-networks'></a> Assign AZs and Networks

Perform the following steps:

1. Click **Assign AZs and Networks**.

    ![Assign AZs and Networks pane in Ops Manager](images/azs-networks.png)

1. Select an availability zone (AZ) for your singleton jobs and one or more AZs to balance other jobs in.
  <p class="note"><strong>Note</strong>: If you upgrade PKS, you must place singleton jobs in the AZ you selected when you first installed the PKS tile.
  You cannot move singleton jobs to another AZ.</p>
  <p class="note"><strong>Note</strong>: In PKS, Pivotal Container Service is a singleton job. This broker VM enables the creation of PKS clusters through the PKS CLI.</p>
1. Under **Network**, select a subnet for the PKS broker.
1. Under **Service Network**, select a subnet for the on-demand service instances created by the PKS broker.
1. Click **Save**.

###<a id='pks-api'></a> PKS API

Perform the following steps:

1. Click **PKS API**.
1. Under **Certificate**, provide your own certificate or have Ops Manager generate one.
To generate a new certificate and key, enter a wildcard domain you own.
For example, `*.pks.pcfgcp.mydomain.com`.
1. Under **Generate RSA Certificate**, provide the domain names that you want your certificate to have.
The domain names should contain the hostname you intend to use for accessing the PKS API service and UAA.
1. Click **Save**.

###<a id='plans'></a> Plans

To activate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
  <p class="note"><strong>Note</strong>: A plan defines a set of resource types used for deploying clusters. You can configure up to three plans.</p>
1. Select **Active** to activate the plan and make it available to developers deploying clusters.
1. Under **Name**, provide a unique name for the plan.
1. Under **Description**, edit the description as needed.
The plan description appears in the Services Marketplace, which developers can access by using PKS CLI.
1. Under **Default Cluster Authorization Mode**, select an authentication mode for the Kubernetes clusters. For more information, see the [RBAC Support in Kubernetes](http://blog.kubernetes.io/2017/04/rbac-support-in-kubernetes.html) blog post.
1. Under **AZ placement**, select an AZ for the Kubernetes clusters deployed by PKS.
1. Under **ETCD/Master VM Type**, select the type of VM to use for Kubernetes etcd and master nodes.
1. Under **Master Persistent Disk Type**, select the size of the persistent disk for the Kubernetes master VM.
1. Under **Worker VM Type**, select the type of VM to use for Kubernetes worker nodes.
1. Under **Worker Persistent Disk Type**, select the size of the persistent disk for the Kubernetes worker nodes.
1. Under **Worker Node Instances**, select the default number of Kubernetes worker nodes to provision for each cluster.
1. Under **Errand VM Type**, select the size of the VM where the errand will run. The smallest instance possible is sufficient, as the only errand running on this VM is the one that applies the **Default Cluster App** YAML configuration.
1. (Optional) Under **(Optional) Add-ons - Use with caution**, enter additional YAML configuration to [add custom workloads](managing.html#custom-workloads) to each cluster in this plan. You can specify multiple files using `---` as a separator.
1. If you want users to be able to create [pods with privileged containers](https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers), select the **Enable Privileged Containers - Use with caution** option.
1. Click **Save**.

To deactivate a plan, perform the following steps:

1. Click the **Plan 1**, **Plan 2**, or **Plan 3** tab.
1. Select **Plan Inactive**.
1. Click **Save**.

###<a id='cloud-provider'></a> Kubernetes Cloud Provider

Perform the following steps:

1. Click **Kubernetes Cloud Provider**.
1. Under **Choose your IaaS**, select **GCP**.
1. Perform the steps specific to GCP.
	* Ensure the values match those in the **Google Config** section of the **Ops Manager** tile:
		1. Enter your **GCP Project Id**, which is the name of the  deployment in your Ops Manager environment. 
		1. Enter your **VPC Network**, which is the VPC network name for your Ops Manager environment.
		1. Enter your **GCP Service Key**, which you created using [the instructions from the GCP prerequisites](gcp-requirements.html).
1. Click **Save**.

###<a id='networking'></a> Networking

Perform the following steps:

1. Click **Networking**.
1. Under **Network**, select the Container Network Interface to use.
	* For **Flannel**, no additional fields are required.
	* For **NSX-T**, see [Installing and Configuring PKS with NSX-T Integration](installing-nsx-t.html).
1. Click **Save**.

###<a id='uaa'></a> UAA

Perform the following steps:

1. Click **UAA**.
1. Enter the URL you use to access UAA.
<p class="note"><strong>Note</strong>: This must match the wildcard domain provided when generating the PKS API certificate in the [PKS API](installing-pks-gcp.html#pks-api) section, or in the SAN for the certificate if you provided your own.</p>
1. Enter the time (in seconds) for the PKS CLI access token lifetime.
1. Enter the time (in seconds) for the PKS CLI refresh token lifetime.

###<a id='syslog'></a> (Optional) Syslog

You can designate an external syslog endpoint for PKS component and cluster log messages.

To specify the destination for PKS log messages, perform the following steps:

1. Click **Syslog**.
1. Select **Yes** to configure syslog forwarding.
1. Enter the destination syslog endpoint.
1. Enter the destination syslog port.
1. Select a transport protocol for log forwarding.
1. (Optional) If you select TLS to forward encrypted logs, perform the following steps:
  1. Provide the accepted fingerprint (SHA1) or name of remote peer. For example, `*.YOUR-LOGGING-SYSTEM.com`.
  1. Provide a TLS certificate for the destination syslog endpoint.
  <p class="note"><strong>Note</strong>: You do not need to provide a new certificate if the TLS certificate for the destination syslog endpoint is signed by a Certificate Authority (CA) in your BOSH certificate store.</p>

###<a id='errands'></a> Errands

Errands are scripts that run at designated points during an installation. 

To configure when post-deploy and pre-delete errands for PKS are run, make a selection in the dropdown next to the errand. For a typical PKS deployment, Pivotal recommends that you leave the default settings.

For more information about errands and their configuration state, see [Managing Errands in Ops Manager](https://docs.pivotal.io/pivotalcf/customizing/managing_errands.html).

###<a id='resource-config'></a> (Optional) Resource Config

To modify the resource usage of PKS, click **Resource Config** and edit the **PKS on-demand broker** job.

###<a id='stemcell'></a> (Optional) Stemcell

To edit the stemcell configuration, click **Stemcell**. Click **Import Stemcell** to import a new stemcell. 

PKS uses floating stemcells. Floating stemcells allow upgrades to the minor versions of stemcells but not the major versions. 
For example, a stemcell can float from `1234.56` to `1234.99` but not from `1234.991` to `1235.0`. 
For more information on floating stemcells, see the 
[Understanding Floating Stemcells](http://docs.pivotal.io/pivotalcf/customizing/understanding-stemcells.html) topic.

<p class="note warning"><strong>WARNING</strong>: Because PKS uses floating stemcells, updating the PKS tile with a new stemcell triggers the rolling of every VM in each cluster. Also, updating other product tiles in your deployment with a new stemcell causes the PKS tile to roll VMs. This rolling is enabled by the <b>Upgrade all clusters errand</b>. Pivotal recommends that you keep this errand turned on because automatic rolling of VMs ensures that all deployed cluster VMs are patched. However, automatic rolling can cause downtime in your deployment.</p>

##<a id='apply-changes'></a> Step 3: Apply Changes

After configuring the tile, return to the Ops Manager Installation Dashboard and click **Apply Changes** to deploy the tile.

##<a id='retrieve-pks-api'></a> Step 4: Retrieve PKS API Endpoint

You must share the PKS API endpoint to allow your organization to use the API to create, update, and delete clusters.

When an operator creates a cluster, they provide an IP address for the Kubernetes master host, then point the load balancer to the newly created cluster.
If you use a load balancer as a service (LBaaS) tool, your LBaaS may manage cluster creation and configuration.

See [Using PKS](using.html#create-cluster) for more information.

Perform the following steps to retrieve the PKS API endpoint:

1. Navigate to the Ops Manager Installation Dashboard. 
1. Click the PKS tile.
1. Click the **Status** tab and locate the IP address of the PKS API endpoint. This is the endpoint that developers use to create and manage clusters.

##<a id='loadbalancer-pks-api'></a> Step 5: Configure External Load Balancer

Configure your external TCP or HTTPS load balancer to resolve to the domain name used in the certificate you provided during the [PKS API](#pks-api) section of the tile configuration.
Your external load balancer forwards traffic to the PKS API endpoint on port 9021 and the UAA endpoint on port 8443.

The load balancer should be configured with:

- The IP address from [Step 4: Retrieve PKS API Endpoint](installing-pks-gcp.html#retrieve-pks-api)
- Ports 8443 and 9021
- The HTTPS or TCP protocol


