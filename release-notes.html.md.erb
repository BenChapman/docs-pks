---
title: PKS Release Notes
owner: PKS
---

<strong><%= modified_date %></strong>

## <a id="v1.1.0"></a>v1.1.0 RC

**Release Date**: June 5, 2018

### <a id="v1.1-upgrade"></a>Upgrade Procedure

<p class="note"><strong>Note</strong>:
The only supported upgrade path for PKS v1.1.0 is from PKS v1.0.2 and later to PKS v1.1.0. Do not upgrade from PKS v1.0.0 directly to v1.1.0. Instead, upgrade to PKS v1.0.2, then PKS v1.1.0. Alternatively, do a unique install of PKS v1.1.0.
</p>

To upgrade to PKS v1.0.0, follow the procedures in [Upgrade PKS](upgrade-pks.html).

### <a id="v1.1.0-features"></a>Features

* Adds support for backing up and restoring PKS using BOSH Backup and Restore (BBR). See [Backing Up and Restoring PKS](backup-and-restore.html) for more information.
* Adds support for granting PKS control pane access to clients and external LDAP groups. See the [Grant Cluster Access](manage-users.html#cluster-access) section of _Manage Users in UAA_ for more information.
* Adds support for Kubernetes 1.10.2.
* Adds support for node network access via HTTP proxy for vSphere deployments.
* Adds support for PKS integration with [VMware Wavefront](https://www.wavefront.com) to capture metrics for clusters and pods.
* Adds support for PKS integration with [VMware vRealize Log Insight (vRLi](https://www.vmware.com/products/vrealize-log-insight.html) for tagged logging of the control plane, clusters, and pods. 
* Adds support for [Harbor Registry]( https://vmware.github.io/harbor/) integration enhancements: updated Harbor tile, ability to use NFS and Google Buckets as an image store, and HTTP/HTTPS proxy servers for Clair.
* Adds support for allowing workers to be deployed across Availability Zones (AZs).
* Adds support for network automation and node network isolation.
* Adds support for NO-NAT deployment topologies for PKS installations on NSX-T. See [Installing and Configuring PKS with NSX-T Integration](installing-nsx-t.html) for more information.
* Adds support for integration with [(VMware Analytics Cloud (VAC)](https://codepen.io/didkobravo/project/live/AYdRpX) to capture telemetry information.
* Adds support for deploying multiple masters (with multi-etcd instances) across Availability Zones (AZs).
* Improves security by removing ABAC authorization option for clusters.

### <a id="v1.1.0-fixed"></a>Fixed Issues

### <a id="v1.1.0-versions"></a>Component Versions

PKS v1.1.0 RC includes or supports the following component versions:

<table border="1" class="nice">
  <tbody>
    <tr>
    <th>Product Component</th>
    <th>Version Supported</th>
    <th>Notes</th>
   </tr>
   <tr>
   <td>Pivotal Cloud Foundry Operations Manager (Ops Manager)</td>
   <td>2.1</td>
   <td>Separate download available from Pivotal Network</td>
   </tr>
   <tr>
   <td>Stemcell</td>
   <td>3451.25</td>
   <td></td>
   </tr>
   <tr>
   <td>Kubernetes</td>
   <td>1.10.2</td>
   <td>Packaged in the PKS Tile (CFCR)</td>
   </tr>
   <tr>
   <td>CFCR (Kubo)</td>
   <td>0.17</td>
   <td>Packaged in the PKS Tile</td>
   </tr>
   <tr>
   <td>Golang</td>
   <td>1.10</td>
   <td>Packaged in the PKS Tile</td>
   </tr>   
   <tr>
   <td>NCP</td>
   <td>2.2</td>
   <td>Packaged in the PKS Tile</td>
   </tr>
   <tr>
   <td>Kubernetes CLI</td>
   <td>1.10.2</td>
   <td>Separate download available from the PKS section of Pivotal Network</td>
   </tr>
   <tr>
   <td>PKS CLI</td>
   <td>1.1</td>
   <td>Separate download available from the PKS section of Pivotal Network</td>
   </tr>
   <tr>
   <td>VMware vSphere</td>
   <td>6.5 U2, 6.5 U1, and 6.5. Editions: 
   <ul>
   <li>vSphere Enterprise Plus Edition</li>
   <li>vSphere with Operations Management Enterprise Plus</li>
   </ul>
   </td>
   <td>vSphere versions supported for Pivotal Container Service (PKS)</td>
   </tr>
   <td>VMware NSX-T</td>
   <td>2.1 - Advanced Edition</td>
   <td>NST-T versions supported for Pivotal Container Service (PKS)</td>
   </tr>
   <tr>
   <td>VMware Harbor Registry</td>
   <td>1.5.0</td>
   <td>Separate download available from Pivotal Network</td>
   </tr>
   </tbody>
   <tfoot>
   <tr>
    <td colspan="3"><em>* Components marked with an asterisk have been patched to resolve security vulnerabilities or fix component behavior.</em></td>
   </tr>
  </tfoot>
</table>

### <a id="known-issues"></a>Known Issues

This section includes known issues with PKS v1.1.0 RC and corresponding workarounds.

* The PKS v1.1.0 RC is only compatible with stemcell v3541.25. The latest stemcells available on Pivotal Network do not work with this RC release.
* The PKS v1.1.0 RC is not compliant with Kubernetes 1.10 on NSX-T.
* The version of NSX-T multi-master support included in this release has a limit on 2 clusters.
* If you use PKS CLI v1.0.x with PKS tile v1.1.x, you must log in every 600 seconds to manually refresh the CLI token.
Pivotal recommends upgrading to PKS CLI v1.1.x to solve this issue.

#### <a id="target-lb"></a>Reconfigure GCP Load Balancers After Master VM Recreation

If Kubernetes master node VMs are recreated for any reason, you must reconfigure your cluster load balancers to point to the new master VMs.
For example, after a stemcell upgrade, BOSH recreates the VMs in your deployment.

To reconfigure your GCP cluster load balancer to use the new master VM, do the following:

1. <%= partial 'login-pks-api' %>
1. To locate your cluster ID, run `pks cluster CLUSTER-NAME`. Your cluster ID appears in the **UUID** row.
1. Gather credential and IP address information for your BOSH Director, SSH into the Ops Manager VM, and use the BOSH CLI to log in to the BOSH Director from the Ops Manager VM.
For more information, see [Advanced Troubleshooting with the BOSH CLI](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html).
1. Identify the name of your cluster deployment. For example:
  <pre class="terminal">$ bosh -e pks deployments</pre>
  Your cluster deployment name begins with `service-instance` and includes the UUID you located in a previous step.
1. Identify the master VM IDs by listing the VMs in your cluster. For example:
   <pre class="terminal">$ bosh -e pks -d service-instance-aa1234567bc8de9f0a1c vms</pre>
   Your master VM IDs appear in the **VM CID** column.
1. Navigate to the [GCP console](https://console.cloud.google.com/).
1. In the sidebar menu, select **Network Services > Load balancing**.
1. Select your cluster load balancer and click **Edit**.
1. Click **Backend configuration**.
1. Click **Select existing instances**.
1. Select the new master VM IDs from the dropdown.
1. Click **Update**.

For more information about cluster load balancers in GCP, see [Configuring a GCP Load Balancer for PKS Clusters](gcp-cluster-load-balancer.html).

#### <a id="upgrading-abac"></a>Upgrading ABAC Clusters

ABAC is no longer supported in v1.1. All existing ABAC clusters will be migrated to RBAC. When upgrading these clusters, kube-system pods will experience a small period of downtime. Additionally, any workloads using service accounts in ABAC will stop working after the upgrade. To fix these workloads, they must use RBAC.
